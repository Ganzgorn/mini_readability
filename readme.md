<h3>Запускается так:</h3>
python rability.py http://www.gazeta.ru/business/news/2016/03/11/n_8357951.shtml http://lenta.ru/articles/2016/03/10/chucknorris/

<h3>Среда выполнения:</h3>
Работает под windows, linux (проверялось)<br>
Python 3.4<br>
requirements.txt содержит зависимости (lxml)<br>


<h2>Формулировка задачи</h2>

Большинство веб-страниц сейчас перегружено всевозможной рекламой... Наша задача «вытащить»
из веб-страницы только полезную информацию, отбросив весь «мусор» (навигацию, рекламу и тд).
Полученный текст нужно отформатировать для максимально комфортного чтения в любом
текстовом редакторе. <br>
<b>Правила форматирования:</b> ширина строки не больше 80 символов (если
больше, переносим по словам), абзацы и заголовки отбиваются пустой строкой. Если в тексте
встречаются ссылки, то URL вставить в текст в квадратных скобках. Остальные правила на ваше
усмотрение.<br>

Программа оформляется в виде утилиты командной строки, которой в качестве параметра
указывается произвольный URL. Она извлекает по этому URL страницу, обрабатывает ее и
формирует текстовый файл с текстом статьи, представленной на данной странице.
В качестве примера можно взять любую статью на lenta.ru, gazeta.ru и тд
Алгоритм должен быть максимально универсальным, то есть работать на большинстве сайтов.

<b>Усложнение задачи 1:</b> Имя выходного файла должно формироваться автоматически по URL.<br>
Примерно так:
http://lenta.ru/news/2013/03/dtp/index.html => [CUR_DIR]/lenta.ru/news/2013/03/dtp/index.txt<br>
<b>Усложнение задачи 2:</b> Программа должна поддаваться настройке – в отдельном файле/файлах
задаются шаблоны обработки страниц.

<h4>Требования к выполнению задачи</h4>
1. Задача выполняется на С++|Python с использованием классов. Не должно использоваться
сторонних библиотек, впрямую решающих задачу.<br>
2. Предпочтительная среда выполнения – MS Windows.<br>
3. Решение должно состоять из документа, описывающего алгоритм, исходных кодов
программы, исполняемого модуля.<br>
4. Приложите список URL, на которых вы проверяли свое решение. И результаты проверки.<br>
5. Желательно указать направление дальнейшего улучшения/развития программы.<br>

<hr>
<h2>Описание работы программы</h2>
<h4>Алгоритм получения контента:</h4>
1) Парсим информацию по урлу с помощью lxml<br>
2) Из body достаем все тэги div добавляем в сисок подозреваемых<br>
3) Каждому элементу из списка считаем метрики (количество слов, символов, знаков пунктуации, вложенных тегов и т.д.)<br>
4) Производим расчёт стоимости каждого элемента<br>
5) Сортируем список подозреваемых по стоимости элемента, по убыванию.<br>
6) Выбираем первый элемент, получаем его текст и все вложенные теги<br>
7) Все вложенные теги в тексте форматируем согласно конфигурации<br>
8) Форматируем текст согласно конфигурации (длина строк)<br>
9) Возвращаем отформатированный текст<br>

<h4>Алгоритм программы:</h4>
1) Получаем список url в качестве аргументов программы<br>
2) Для каждого url получаем контент<br>
3) Форматируем имя, путь к файлу<br>
4) Создаём файл, записываем контент в файл<br>

<b>Для проверки использовалось несколько url, некоторые из них:</b><br>
http://photo.novokreshenova.ru/schweinfurt/<br>
https://habrahabr.ru/company/mailru/blog/200394/<br>
http://www.gazeta.ru/business/news/2016/03/11/n_8357951.shtml<br>
http://lenta.ru/articles/2016/03/10/chucknorris/<br>

Дальнейшее развитие зависит от области применения.<br>
Но в общем можно выделить несколько доработок. В частности:<br>
1) Написание тестов, чтобы дальнейшие доработки не нарушали правильную работу<br>
2) Поддержка других тегов, типа 'code', 'article', 'q', 'img'<br>
3) Доработка и оптимизация алгоритма<br>
4) Вывод в html формат с подстановкой "своих" тэгов<br>
